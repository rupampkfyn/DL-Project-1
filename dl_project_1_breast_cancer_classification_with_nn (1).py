# -*- coding: utf-8 -*-
"""DL Project 1. Breast Cancer Classification with NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vJFc-u8QUEqiwk90A3J6t-2gs2h1gmTT

Importing the Dependencies
"""

import numpy as np
import pandas as pd
import sklearn.datasets
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

"""Data Collection and Processing"""

# loading the data from sklearn
breast_cancer_dataset = sklearn.datasets.load_breast_cancer()

print(breast_cancer_dataset)

# loading the data to a Pandas Dataframe
data_frame = pd.DataFrame(breast_cancer_dataset.data, columns= breast_cancer_dataset.feature_names)

# printing the first five rows of the dataframe
data_frame.head()

# adding the target column to the dataframe
data_frame['label'] = breast_cancer_dataset.target

# printing the last five rows of the dataframe
data_frame.tail()

# number of rows and columns of the dataset
data_frame.shape

# getting some information about the data
data_frame.info()

# checking for missing values
data_frame.isnull().sum()

# statistical measures about the data
data_frame.describe()

# checking the distribution of the Target Variable
data_frame['label'].value_counts()

"""1 --> Benign

0 --> Malignant
"""

data_frame.groupby('label').mean()

"""Separating the features and target"""

X = data_frame.drop(columns= 'label', axis=1)
Y = data_frame['label']

print(X)

print(Y)

"""Splitting the data into Training data and Test data"""

X_train,X_test,Y_train,Y_test = train_test_split(X,Y, test_size=0.2, random_state = 2)

print(X.shape, X_train.shape,X_test.shape)

"""Standardize the Data"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train_std = scaler.fit_transform(X_train)
X_test_std = scaler.transform(X_test)

"""**Building the Neural Network**"""

# importing Tensorflow and Keras
import tensorflow as tf
tf.random.set_seed(3)
from tensorflow import keras

# setting up the layers of Neural Network

model = keras.Sequential([
                          keras.layers.Flatten(input_shape=(30,)),
                          keras.layers.Dense(20, activation='relu'),
                          keras.layers.Dense(2, activation='sigmoid')
])

# Compile the model
model.compile(
              optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy']
              )

# Train the Neural Network
history = model.fit(X_train_std, Y_train, validation_split=0.1, epochs=10)

"""Visualizing Accuracy and Loss"""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])

plt.title('model acuuracy')
plt.ylabel('accuracy')
plt.xlabel('epochs')

plt.legend(['training data', 'validation data'], loc = 'lower right')

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epochs')

plt.legend(['training data', 'validation data'], loc = 'upper right')

"""Accuracy of the model on Test Data"""

loss, accuracy = model.evaluate(X_test_std, Y_test)
print(accuracy)

print(X_test_std.shape)
print(X_test_std[0])

Y_pred = model.predict(X_test_std)

print(Y_pred.shape)
print(Y_pred[0])

print(X_test_std)

print(Y_pred)

"""model.predict() gives the prediction probability of each class for that data point"""

# argmax function

my_list = [10, 20, 30, 50, 70]

index_of_max_value = np.argmax(my_list)
print(my_list)
print(index_of_max_value)

# converting the prediciton probability to class label
Y_pred_labels = [np.argmax(i) for i in Y_pred]
print(Y_pred_labels)

"""**Building the predictive system**


"""

input_data = (19.81,22.15,130,1260,0.09831,0.1027,0.1479,0.09498,0.1582,0.05395,0.7582,1.017,5.865,112.4,0.006494,0.01893,0.03391,0.01521,0.01356,0.001997,27.32,30.88,186.8,2398,0.1512,0.315,0.5372,0.2388,0.2768,0.07615)
# change the input data to a numpy array
input_data_as_numpy_array = np.asarray(input_data)

# rehsaping the numpy array as we are predicting for one data point
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

# standardizing the input data
input_data_std = scaler.transform(input_data_reshaped)

prediction = model.predict(input_data_std)
print(prediction)

prediction_label = [np.argmax(prediction)]

print(prediction_label)

if (prediction_label[0] == 0):
  print('The tumor is Malignant')
else:
  print('The tumor is Benign')

